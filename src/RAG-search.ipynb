{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# RAG Search and Retrieval","metadata":{}},{"cell_type":"markdown","source":"## Environment Setup","metadata":{}},{"cell_type":"code","source":"%%capture\n\n# installations\n!pip install --quiet sentence_transformers transformers torch peft huggingface_hub kaggle pinecone lark rank_bm25 langchain_huggingface langchain_experimental langchain_pinecone \n\n# THE REGS\nimport pandas as pd\nimport numpy as np\nimport kagglehub\nimport torch\nimport os\nimport time\n\n# Transformers\nfrom transformers import AutoTokenizer, AutoModel\nimport torch\n\n# PINECONE\nfrom pinecone import Pinecone\nfrom pinecone import ServerlessSpec\n\n# LANGCHAIN\nfrom langchain_huggingface import HuggingFaceEmbeddings\nfrom langchain_pinecone import PineconeVectorStore\nfrom langchain_core.documents import Document\n\n# Kaggle environment\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Get model\nmodel_name = \"BAAI/bge-large-zh-v1.5\"\nmodel = AutoModel.from_pretrained(model_name)\n\nprint(\"About the model: \\n\\n\", model.config, \"\\n\")\n\n# Get corresponding tokenizer/encoder\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\nprint(\"About the tokenizer: \\n\\n\", tokenizer)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# get access to populated index\npc = Pinecone(api_key=user_secrets.get_secret(\"PINECONE_API_KEY\"))\nindex_name = \"book-vector-store\"\nindex = pc.Index(index_name)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# create embedding object using the model\nmodel_kwargs = {'device': 'cpu'}\nencode_kwargs = {'normalize_embeddings': False}\nembedder = HuggingFaceEmbeddings(\n    model_name=model_name,\n    model_kwargs=model_kwargs,\n    encode_kwargs=encode_kwargs\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"## create PineconeVectorStore object\nvector_store = PineconeVectorStore(index=index, embedding=embedder)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Test out the search\n\n> A similarity_search on a PineconeVectorStore object returns a list of LangChain Document objects most similar to the query provided. While the similarity_search uses a Pinecone query to find the most similar results, this method includes additional steps and returns results of a different type.The similarity_search method accepts raw text and automatically embeds it using the Embedding object provided when you initialized the PineconeVectorStore. You can also provide a k value to determine the number of LangChain Document objects to return. The default value is k=4.\n","metadata":{}},{"cell_type":"code","source":"## Example\n#query = \"Who is Ketanji Brown Jackson?\"\n    #vectorstore.similarity_search(query)\n    \n    # Response:\n    # [\n    #    Document(page_content='Ketanji Onyika Brown Jackson is an American lawyer and jurist who is an associate justice of the Supreme Court of the United...', metadata={'chunk': 0.0, 'source': 'https://en.wikipedia.org/wiki/Ketanji_Brown_Jackson', 'title': 'Ketanji Brown Jackson', 'wiki-id': '6573'}),  \n    #    Document(page_content='Jackson was nominated to the Supreme Court by President Joe Biden on February 25, 2022, and confirmed by the U.S. Senate...', metadata={'chunk': 1.0, 'source': 'https://en.wikipedia.org/wiki/Ketanji_Brown_Jackson', 'title': 'Ketanji Brown Jackson', 'wiki-id': '6573'}),  \n    #    Document(page_content='Jackson grew up in Miami and attended Miami Palmetto Senior High School. She distinguished herself as a champion debater...', metadata={'chunk': 3.0, 'source': 'https://en.wikipedia.org/wiki/Ketanji_Brown_Jackson', 'title': 'Ketanji Brown Jackson', 'wiki-id': '6573'}),\n    #    Document(page_content='After high school, Jackson matriculated at Harvard University to study government, having applied despite her guidance...', metadata={'chunk': 5.0, 'source': 'https://en.wikipedia.org/wiki/Ketanji_Brown_Jackson', 'title': 'Ketanji Brown Jackson', 'wiki-id': '6573'})\n    # ]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"You can also optionally apply a metadata filter to your similarity search. The filtering query language is the same as for Pinecone queries, as detailed in [Filtering with metadata](https://docs.pinecone.io/guides/index-data/indexing-overview#metadata).","metadata":{}},{"cell_type":"code","source":"## Example\n   # query = \"Tell me more about Ketanji Brown Jackson.\"\n   #  vectorstore.similarity_search(query, filter={'source': 'https://en.wikipedia.org/wiki/Ketanji_Brown_Jackson'})","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Test out RAG with Langchain\nIn RAG, you take the query as a question that is to be answered by a LLM, but the LLM must answer the question based on the information it is seeing from the vectorstore.","metadata":{}},{"cell_type":"code","source":"## Example\n# from langchain_openai import ChatOpenAI  \n# from langchain.chains import RetrievalQA  \n# # completion llm  \n# llm = ChatOpenAI(  \n#     openai_api_key=OPENAI_API_KEY,  \n#     model_name='gpt-3.5-turbo',  \n#     temperature=0.0  \n# )  \n# qa = RetrievalQA.from_chain_type(  \n#     llm=llm,  \n#     chain_type=\"stuff\",  \n#     retriever=vectorstore.as_retriever()  \n# )  \n# qa.invoke(query)  \n\n# Response:\n# Benito Mussolini was an Italian politician and journalist who served as the Prime Minister of Italy from 1922 until 1943. He was the leader of the National Fascist Party and played a significant role in the rise of fascism in Italy...","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}