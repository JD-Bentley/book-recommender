{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":22796,"sourceType":"datasetVersion","datasetId":17368,"isSourceIdPinned":false}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Vector DB Setup ","metadata":{}},{"cell_type":"markdown","source":"## Environment Setup","metadata":{}},{"cell_type":"code","source":"%%capture\n\n# installations\n!pip install --quiet sentence_transformers transformers torch peft huggingface_hub kaggle pinecone lark rank_bm25 langchain_huggingface langdetect langchain_experimental langchain_pinecone \n\n# THE REGS\nimport pandas as pd\nimport numpy as np\nimport kagglehub\nimport torch\nimport string\nimport os\nimport time\nimport re\n\n# NLP\nimport nltk\nfrom nltk.tokenize import sent_tokenize\nnltk.download('punkt')\nfrom langdetect import detect, DetectorFactory\n\n# Transformers\nfrom transformers import AutoTokenizer, AutoModel\nimport torch\n\n# PINECONE\nfrom pinecone import Pinecone\nfrom pinecone import ServerlessSpec\n\n# LANGCHAIN\nfrom langchain_huggingface import HuggingFaceEmbeddings\nfrom langchain_pinecone import PineconeVectorStore\nfrom langchain_core.documents import Document\n\n# Kaggle environment\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-02T20:06:29.341488Z","iopub.execute_input":"2025-09-02T20:06:29.341843Z","iopub.status.idle":"2025-09-02T20:06:43.963784Z","shell.execute_reply.started":"2025-09-02T20:06:29.341817Z","shell.execute_reply":"2025-09-02T20:06:43.962622Z"}},"outputs":[],"execution_count":1},{"cell_type":"markdown","source":"## Data Setup","metadata":{}},{"cell_type":"code","source":"# Reformat data file so it fits into a pandas dataframe\ndef text_to_csv_pandas(input_file, output_file, column_names, delimiter=None):\n    \"\"\"\n    Reads a text file into a Pandas DataFrame and saves it as a CSV file.\n\n    Args:\n        input_file (str): The path to the input text file.\n        output_file (str): The path to the output CSV file.\n        delimiter (str, optional): The delimiter used in the text file. Defaults to None, \n        which will split each line by whitespace if the text file is not delimited.\n    \"\"\"\n    if delimiter is not None:\n        df = pd.read_csv(input_file, sep=delimiter, names = column_names, header=None)\n    else:\n         df = pd.read_csv(input_file, sep=r'\\s+', names = column_names, header=None)\n    df.to_csv(output_file, index=False, header=True)\n\n# Columns in the data set\ncolumns = ['Wikipedia article ID', \n           'Freebase ID', \n           'Book title', \n           'Author', \n           'Publication date', \n           'Book genres', \n           'Plot summary']\n\ntext_to_csv_pandas('/kaggle/input/cmu-book-summary-dataset/booksummaries.txt', 'data.csv', \n                   column_names = columns, delimiter='\\t')\n\ndata = pd.read_csv('/kaggle/working/data.csv')\n\n# drop the ID columns\ndata.drop(columns=['Wikipedia article ID', 'Freebase ID'], inplace=True)\n\n# preview\ndata.head(n=3)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-02T20:06:43.965203Z","iopub.execute_input":"2025-09-02T20:06:43.966007Z","iopub.status.idle":"2025-09-02T20:06:47.161231Z","shell.execute_reply.started":"2025-09-02T20:06:43.965959Z","shell.execute_reply":"2025-09-02T20:06:47.160369Z"}},"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"           Book title           Author Publication date  \\\n0         Animal Farm    George Orwell       1945-08-17   \n1  A Clockwork Orange  Anthony Burgess             1962   \n2          The Plague     Albert Camus             1947   \n\n                                         Book genres  \\\n0  {\"/m/016lj8\": \"Roman \\u00e0 clef\", \"/m/06nbt\":...   \n1  {\"/m/06n90\": \"Science Fiction\", \"/m/0l67h\": \"N...   \n2  {\"/m/02m4t\": \"Existentialism\", \"/m/02xlf\": \"Fi...   \n\n                                        Plot summary  \n0   Old Major, the old boar on the Manor Farm, ca...  \n1   Alex, a teenager living in near-future Englan...  \n2   The text of The Plague is divided into five p...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Book title</th>\n      <th>Author</th>\n      <th>Publication date</th>\n      <th>Book genres</th>\n      <th>Plot summary</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Animal Farm</td>\n      <td>George Orwell</td>\n      <td>1945-08-17</td>\n      <td>{\"/m/016lj8\": \"Roman \\u00e0 clef\", \"/m/06nbt\":...</td>\n      <td>Old Major, the old boar on the Manor Farm, ca...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>A Clockwork Orange</td>\n      <td>Anthony Burgess</td>\n      <td>1962</td>\n      <td>{\"/m/06n90\": \"Science Fiction\", \"/m/0l67h\": \"N...</td>\n      <td>Alex, a teenager living in near-future Englan...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>The Plague</td>\n      <td>Albert Camus</td>\n      <td>1947</td>\n      <td>{\"/m/02m4t\": \"Existentialism\", \"/m/02xlf\": \"Fi...</td>\n      <td>The text of The Plague is divided into five p...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":2},{"cell_type":"markdown","source":"## Data Cleaning","metadata":{}},{"cell_type":"code","source":"print(\"Number of NA values for each feature:\\n\", data.isna().sum())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-02T20:06:47.162174Z","iopub.execute_input":"2025-09-02T20:06:47.162538Z","iopub.status.idle":"2025-09-02T20:06:47.177933Z","shell.execute_reply.started":"2025-09-02T20:06:47.162515Z","shell.execute_reply":"2025-09-02T20:06:47.176909Z"}},"outputs":[{"name":"stdout","text":"Number of NA values for each feature:\n Book title             0\nAuthor              2382\nPublication date    5610\nBook genres         3718\nPlot summary           0\ndtype: int64\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"### Genre Column\n* Remove brackets\n* Remove /m/ char sequences\n* Fix utf-8 symbols\n* Put genres in list format for each cell","metadata":{}},{"cell_type":"code","source":"# CLEAN GENRE COLUMN\n\n# Find all genres in cell block, do not keep char sequences that begin with /m/\ndata['Book genres'] = data['Book genres'].apply(lambda row: re.findall(r'\":\\s*\"([^\"]+)\"', str(row)))\n\n# take care of utf-8 symbols as well (ex. \\\\u00e0)\ndata['Book genres'] = data['Book genres'].apply(lambda cell: [bytes(word, \"utf-8\").decode(\"unicode_escape\") for word in cell])\n\n# Preview\ndata.head(n=3)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-02T20:06:47.179335Z","iopub.execute_input":"2025-09-02T20:06:47.179829Z","iopub.status.idle":"2025-09-02T20:06:47.282988Z","shell.execute_reply.started":"2025-09-02T20:06:47.179794Z","shell.execute_reply":"2025-09-02T20:06:47.281775Z"}},"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"           Book title           Author Publication date  \\\n0         Animal Farm    George Orwell       1945-08-17   \n1  A Clockwork Orange  Anthony Burgess             1962   \n2          The Plague     Albert Camus             1947   \n\n                                         Book genres  \\\n0  [Roman à clef, Satire, Children's literature, ...   \n1  [Science Fiction, Novella, Speculative fiction...   \n2  [Existentialism, Fiction, Absurdist fiction, N...   \n\n                                        Plot summary  \n0   Old Major, the old boar on the Manor Farm, ca...  \n1   Alex, a teenager living in near-future Englan...  \n2   The text of The Plague is divided into five p...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Book title</th>\n      <th>Author</th>\n      <th>Publication date</th>\n      <th>Book genres</th>\n      <th>Plot summary</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Animal Farm</td>\n      <td>George Orwell</td>\n      <td>1945-08-17</td>\n      <td>[Roman à clef, Satire, Children's literature, ...</td>\n      <td>Old Major, the old boar on the Manor Farm, ca...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>A Clockwork Orange</td>\n      <td>Anthony Burgess</td>\n      <td>1962</td>\n      <td>[Science Fiction, Novella, Speculative fiction...</td>\n      <td>Alex, a teenager living in near-future Englan...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>The Plague</td>\n      <td>Albert Camus</td>\n      <td>1947</td>\n      <td>[Existentialism, Fiction, Absurdist fiction, N...</td>\n      <td>The text of The Plague is divided into five p...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":4},{"cell_type":"markdown","source":"### Publication date column\n* The majority of the books half the publication year, therefore we will create a new column for it.","metadata":{}},{"cell_type":"code","source":"# regex to find dates that only contain the year\nregex = r'\\d{4}$'\nyear_only_dates = data[data['Publication date'].str.contains(regex) == True].index\nprint(\"Number of books with only the publication year: \", len(year_only_dates))\n\n# regex to find dates that only contain year and month\nregex = r'\\d{4}-\\d{2}$'\nhalf_dates = data[data['Publication date'].str.contains(regex) == True].index\nprint(\"Number of books with just the year and month: \", len(half_dates))\n\n# regex to find complete dates\nregex = r'\\d{4}-\\d{2}-\\d{2}'\nfull_dates = data[data['Publication date'].str.contains(regex) == True].index\nprint(\"Number of books with the full publication date: \", len(full_dates))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-02T20:06:47.284138Z","iopub.execute_input":"2025-09-02T20:06:47.284541Z","iopub.status.idle":"2025-09-02T20:06:47.325550Z","shell.execute_reply.started":"2025-09-02T20:06:47.284505Z","shell.execute_reply":"2025-09-02T20:06:47.324204Z"}},"outputs":[{"name":"stdout","text":"Number of books with only the publication year:  6799\nNumber of books with just the year and month:  1479\nNumber of books with the full publication date:  2671\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# CREATE NEW COLUMN FOR PUBLICATION YEAR\n\n# New column for publication year\ndata['Publication year'] = 0\n\n# Fill in column with year values\ndata.loc[full_dates, 'Publication year'] = data.loc[full_dates, 'Publication date'].str.split(\"-\").str[0].astype('int')\ndata.loc[half_dates, 'Publication year'] = data.loc[half_dates, 'Publication date'].str.split(\"-\").str[0].astype('int')\ndata.loc[year_only_dates, 'Publication year'] = data.loc[year_only_dates, 'Publication date'].str.split(\"-\").str[0].astype('int')\n\n# Preview\ndata.head(n=3)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-02T20:06:47.328969Z","iopub.execute_input":"2025-09-02T20:06:47.329340Z","iopub.status.idle":"2025-09-02T20:06:47.364482Z","shell.execute_reply.started":"2025-09-02T20:06:47.329307Z","shell.execute_reply":"2025-09-02T20:06:47.363345Z"}},"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"           Book title           Author Publication date  \\\n0         Animal Farm    George Orwell       1945-08-17   \n1  A Clockwork Orange  Anthony Burgess             1962   \n2          The Plague     Albert Camus             1947   \n\n                                         Book genres  \\\n0  [Roman à clef, Satire, Children's literature, ...   \n1  [Science Fiction, Novella, Speculative fiction...   \n2  [Existentialism, Fiction, Absurdist fiction, N...   \n\n                                        Plot summary  Publication year  \n0   Old Major, the old boar on the Manor Farm, ca...              1945  \n1   Alex, a teenager living in near-future Englan...              1962  \n2   The text of The Plague is divided into five p...              1947  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Book title</th>\n      <th>Author</th>\n      <th>Publication date</th>\n      <th>Book genres</th>\n      <th>Plot summary</th>\n      <th>Publication year</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Animal Farm</td>\n      <td>George Orwell</td>\n      <td>1945-08-17</td>\n      <td>[Roman à clef, Satire, Children's literature, ...</td>\n      <td>Old Major, the old boar on the Manor Farm, ca...</td>\n      <td>1945</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>A Clockwork Orange</td>\n      <td>Anthony Burgess</td>\n      <td>1962</td>\n      <td>[Science Fiction, Novella, Speculative fiction...</td>\n      <td>Alex, a teenager living in near-future Englan...</td>\n      <td>1962</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>The Plague</td>\n      <td>Albert Camus</td>\n      <td>1947</td>\n      <td>[Existentialism, Fiction, Absurdist fiction, N...</td>\n      <td>The text of The Plague is divided into five p...</td>\n      <td>1947</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":6},{"cell_type":"markdown","source":"### Plot summary column\n* Remove books with no proper description of the plot.","metadata":{}},{"cell_type":"code","source":"# DELETE BOOKS THAT ARE NOT IN ENGLISH\nDetectorFactory.seed = 0  # for consistent results\n\ndef detect_language(text):\n    try:\n        lang = detect(text)\n        return lang\n    except Exception as e:\n        return f\"Error: {str(e)}\"\n\nlangs = data['Plot summary'].apply(lambda x: detect_language(x))\neng_books = langs[langs == 'en'].index\n\nprint(\"Number of english summaries in dataset: \", len(eng_books))\n\ndata = data.loc[eng_books]\ndata.reset_index(drop=True, inplace=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-02T20:06:47.365642Z","iopub.execute_input":"2025-09-02T20:06:47.365989Z","iopub.status.idle":"2025-09-02T20:08:53.485331Z","shell.execute_reply.started":"2025-09-02T20:06:47.365963Z","shell.execute_reply":"2025-09-02T20:08:53.484116Z"}},"outputs":[{"name":"stdout","text":"Number of english summaries in dataset:  16520\n","output_type":"stream"}],"execution_count":7},{"cell_type":"markdown","source":"#### Reduce number of books to upload to pinecone vectorstore","metadata":{}},{"cell_type":"code","source":"# Function to count sentences using NLTK\ndef count_sentences_nltk(text):\n    sentences = sent_tokenize(text)\n    return len(sentences)\n\n# Filter rows with less than a 6 sentences\nfiltered_df = data[data['Plot summary'].apply(count_sentences_nltk) < 6]\n\n# Filter out rows where genre is not an empty list\nsample_data = filtered_df[filtered_df['Book genres'].apply(lambda x: len(x) > 0)]\n\nsample_data.info()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-02T20:08:53.486763Z","iopub.execute_input":"2025-09-02T20:08:53.487103Z","iopub.status.idle":"2025-09-02T20:09:00.266780Z","shell.execute_reply.started":"2025-09-02T20:08:53.487075Z","shell.execute_reply":"2025-09-02T20:09:00.265216Z"}},"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nIndex: 3245 entries, 8 to 16512\nData columns (total 6 columns):\n #   Column            Non-Null Count  Dtype \n---  ------            --------------  ----- \n 0   Book title        3245 non-null   object\n 1   Author            3004 non-null   object\n 2   Publication date  2316 non-null   object\n 3   Book genres       3245 non-null   object\n 4   Plot summary      3245 non-null   object\n 5   Publication year  3245 non-null   int64 \ndtypes: int64(1), object(5)\nmemory usage: 177.5+ KB\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"# get number of words per plot summary\nwords_per_summary = sample_data['Plot summary'].apply(lambda x: len(nltk.word_tokenize(x)))\n\n# take a look at what these summaries look like (less than 10 words)\nsample_data.loc[words_per_summary[words_per_summary < 10].index].head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-02T20:09:00.269168Z","iopub.execute_input":"2025-09-02T20:09:00.269958Z","iopub.status.idle":"2025-09-02T20:09:01.880057Z","shell.execute_reply.started":"2025-09-02T20:09:00.269913Z","shell.execute_reply":"2025-09-02T20:09:01.878670Z"}},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":"### Since there are multiple valid plot summaries that have 2+ equal signs, we will need to remove them.","metadata":{}},{"cell_type":"code","source":"# Replace substrings with 2+ equal signs\nsample_data.loc[:, 'Plot summary'] = sample_data['Plot summary'].str.replace(r'={2,}', '', regex=True)\n\n# drop rows that have 5 terms or less\nsample_data.drop(words_per_summary[words_per_summary < 6].index.tolist(), inplace=True)\n\n# drop rows that contain \"Plot outline description\" \nsample_data.drop(sample_data[sample_data['Plot summary'].str.contains(\"Plot outline description\", regex=True)].index.tolist(), inplace=True)\n\n# additional text cleaning\nsample_data.loc[:, 'Plot summary'] = sample_data['Plot summary'].apply(lambda x: x.replace('#', ''))\nsample_data.loc[:, 'Plot summary'] = sample_data['Plot summary'].apply(lambda x: x.replace('*', ''))\n\n# dropping rows does not automatically reset index. So we must do this manually.\nsample_data.reset_index(drop=True, inplace=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-02T20:09:01.901118Z","iopub.execute_input":"2025-09-02T20:09:01.901730Z","iopub.status.idle":"2025-09-02T20:09:02.026065Z","shell.execute_reply.started":"2025-09-02T20:09:01.901676Z","shell.execute_reply":"2025-09-02T20:09:02.024766Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_562/2269178130.py:5: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  sample_data.drop(words_per_summary[words_per_summary < 6].index.tolist(), inplace=True)\n/tmp/ipykernel_562/2269178130.py:8: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  sample_data.drop(sample_data[sample_data['Plot summary'].str.contains(\"Plot outline description\", regex=True)].index.tolist(), inplace=True)\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"# pinecone does not accept NaN values\n# Replace NaN in the 'name' column with 'Unknown'\nsample_data.loc[:, 'Publication date'] = sample_data['Publication date'].astype(str)\nsample_data.loc[:, 'Publication date'] = sample_data['Publication date'].fillna('Unknown')\nsample_data.loc[:, 'Author'] = sample_data['Author'].fillna('Unknown')\nsample_data.loc[:, 'Publication date'] = sample_data['Publication date'].replace(['nan'], 'Unknown')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-02T20:09:02.027274Z","iopub.execute_input":"2025-09-02T20:09:02.027596Z","iopub.status.idle":"2025-09-02T20:09:02.041098Z","shell.execute_reply.started":"2025-09-02T20:09:02.027557Z","shell.execute_reply":"2025-09-02T20:09:02.039482Z"}},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":"### Note:\n* Stopwords are not removed as part of the text cleaning process because we are using an LLM to embed-- which are already trained with stopwords and therefore handle the importance of them internally.","metadata":{}},{"cell_type":"markdown","source":"### First we determine which embedding model we will use. \n* We will use BAAI's tuned BERT model.\n* The bge-base-en model is tuned for short queries -- long contexts.","metadata":{}},{"cell_type":"code","source":"# Get model\nmodel_name = \"BAAI/bge-base-en\"\nmodel = AutoModel.from_pretrained(model_name)\n\nprint(\"About the model: \\n\\n\", model.config, \"\\n\")\n\n# Get corresponding tokenizer/encoder\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\nprint(\"About the tokenizer: \\n\\n\", tokenizer)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-02T20:09:31.140058Z","iopub.execute_input":"2025-09-02T20:09:31.140519Z","iopub.status.idle":"2025-09-02T20:09:38.319809Z","shell.execute_reply.started":"2025-09-02T20:09:31.140488Z","shell.execute_reply":"2025-09-02T20:09:38.318827Z"}},"outputs":[{"name":"stderr","text":"2025-09-02 20:09:33.830938: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1756843773.865790     562 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1756843773.877115     562 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"About the model: \n\n BertConfig {\n  \"_attn_implementation_autoset\": true,\n  \"architectures\": [\n    \"BertModel\"\n  ],\n  \"attention_probs_dropout_prob\": 0.1,\n  \"classifier_dropout\": null,\n  \"gradient_checkpointing\": false,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 768,\n  \"id2label\": {\n    \"0\": \"LABEL_0\"\n  },\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 3072,\n  \"label2id\": {\n    \"LABEL_0\": 0\n  },\n  \"layer_norm_eps\": 1e-12,\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"bert\",\n  \"num_attention_heads\": 12,\n  \"num_hidden_layers\": 12,\n  \"pad_token_id\": 0,\n  \"position_embedding_type\": \"absolute\",\n  \"torch_dtype\": \"float32\",\n  \"transformers_version\": \"4.51.3\",\n  \"type_vocab_size\": 2,\n  \"use_cache\": true,\n  \"vocab_size\": 30522\n}\n \n\nAbout the tokenizer: \n\n BertTokenizerFast(name_or_path='BAAI/bge-base-en', vocab_size=30522, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True, added_tokens_decoder={\n\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n}\n)\n","output_type":"stream"}],"execution_count":13},{"cell_type":"markdown","source":"### How Embeddings Operate\nModel: https://huggingface.co/BAAI/bge-base-en\n\n**Encoding**:\n* This is a broad term that refers to the process of transforming data from one format to another. Examples include converting text into binary format, converting characters to numerical values, or compressing data. [[1]](https://medium.com/@pratiyush1/understanding-different-types-of-encoding-and-decoding-in-programming-with-practical-examples-dcbdd5215605#:~:text=Practical%20Example%201:%20Email%20Attachments%20Base64%20encoding,were%20traditionally%20designed%20to%20handle%20text%2Donly%20data)\n  \n**Tokenization**:\n* In the context of natural language processing (NLP), tokenization is a specific type of encoding where text is broken down into smaller units called tokens. These tokens can be words, characters, or even sub-word units. [[2]](https://www.datacamp.com/blog/what-is-tokenization#:~:text=Training%20more%20people?,which%20are%20easier%20to%20analyze)\n\n**Embeddings**:\n* are advanced vector representations of tokens. They try to capture the most nuance, connections, and semantic meanings between tokens. Each embedding is generally a series of real numbers on a vector space computed by a neural network. [[3]](https://medium.com/the-research-nest/explained-tokens-and-embeddings-in-llms-69a16ba5db33)\n\n\n> In short, text is converted to tokens. Tokens are assigned token IDs. These token IDs can be used to create embeddings for more nuanced numerical representation in complex models.\n>\n> Why are embeddings so large and complex? What do they signify?\n>\n> Each token’s embedding is a high-dimensional vector. This allows the model to capture a wide range of linguistic features and nuances, like the meaning of a word, its part of speech, and its relationship to other words in the sentence.\n>\n> * Contextual Embeddings: Unlike simpler word embeddings (like Word2Vec), BERT’s embeddings are contextual. This means the same word can have different embeddings based on its context (its surrounding words). The embeddings need to be rich and complex to capture this contextual nuance.\n> \n> * In more complex models like BERT, you get the final embeddings and access to the embeddings from each layer of the neural network. Each layer captures different aspects of the language, adding to the complexity and size of the tensor.\n>\n> * Input for Further Tasks: These embeddings are used as input for various NLP tasks like sentiment analysis, question answering, and language translation. The richness of the embeddings allows the model to perform these tasks with a high degree of sophistication.\n>\n> * Model’s Internal Representation: The complexity of these tensors reflects how the model ‘understands’ language. Each dimension in the embedding can represent some abstract language feature the model learned during its training.\n> [[3]](https://medium.com/the-research-nest/explained-tokens-and-embeddings-in-llms-69a16ba5db33)","metadata":{}},{"cell_type":"markdown","source":"## VectorStore Setup\nUseful links:\n* [Pinecone](https://docs.pinecone.io/integrations/langchain)\n* [Langchain](https://python.langchain.com/api_reference/pinecone/vectorstores/langchain_pinecone.vectorstores.PineconeVectorStore.html)","metadata":{}},{"cell_type":"markdown","source":"### Step 1. Initialize a vector store\n\nTo securely handle Pinecone API keys and prevent their exposure, especially when sharing code or deploying application:\n\n* Store the Pinecone API key as an environment variable on your system or server.\n\n* Access this variable within your code using the appropriate method for your programming language (e.g., os.environ.get(\"PINECONE_API_KEY\") in Python).\n\nThis keeps the key separate from your codebase and prevents it from being committed to version control.","metadata":{}},{"cell_type":"code","source":"pc = Pinecone(api_key=user_secrets.get_secret(\"PINECONE_API_KEY\"))\n\nindex_name = \"book-vector-store\"\n\nexisting_indexes = [index_info[\"name\"] for index_info in pc.list_indexes()]\n\nif index_name not in existing_indexes:\n    pc.create_index(\n        name=index_name,\n        dimension=1024, # set to embedder's output size\n        metric=\"cosine\",\n        spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\"),\n        deletion_protection=\"enabled\",  # Defaults to \"disabled\"\n    )\n    while not pc.describe_index(index_name).status[\"ready\"]:\n        time.sleep(1)\n\nindex = pc.Index(index_name)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-02T20:09:38.321536Z","iopub.execute_input":"2025-09-02T20:09:38.322444Z","iopub.status.idle":"2025-09-02T20:09:41.267767Z","shell.execute_reply.started":"2025-09-02T20:09:38.322417Z","shell.execute_reply":"2025-09-02T20:09:41.266550Z"}},"outputs":[],"execution_count":14},{"cell_type":"markdown","source":"### Step 2. Prepare/Create documents for the vector store","metadata":{}},{"cell_type":"code","source":"# create documents\nbook_docs = []\n\ndef chunk_list(lst, chunk_size):\n    \"\"\"Splits lst into chunks with length <= chunk_size\"\"\"\n    return [lst[i:i + chunk_size] for i in range(0, len(lst), chunk_size)]\n    \nfor i in range(len(sample_data)):\n    title = sample_data['Book title'].iloc[i]\n    authors = sample_data['Author'].iloc[i]\n    pub_date = sample_data['Publication date'].iloc[i]\n    pub_year = sample_data['Publication year'].iloc[i]\n    genres = sample_data['Book genres'].iloc[i]\n    doc_id = i\n\n    plot_summary_tokens = tokenizer.tokenize(sample_data['Plot summary'].iloc[i])\n    tokenized_chunks = chunk_list(plot_summary_tokens, 512)\n    text_chunks = [tokenizer.convert_tokens_to_string(chunk) for chunk in tokenized_chunks]\n\n    documents = [\n        Document(page_content=text_chunk, \n                 metadata={\"Title\": title, \"Author(s)\": authors, \"Publication Date\": pub_date, \n                           \"Publication year\": str(pub_year), \"Genre(s)\": genres, \"doc_id\": doc_id, \"chunk\": str(i)})\n        for i, text_chunk in enumerate(text_chunks)\n    ]\n\n    book_docs += documents","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-02T20:09:41.268818Z","iopub.execute_input":"2025-09-02T20:09:41.269119Z","iopub.status.idle":"2025-09-02T20:09:43.761512Z","shell.execute_reply.started":"2025-09-02T20:09:41.269096Z","shell.execute_reply":"2025-09-02T20:09:43.760572Z"}},"outputs":[],"execution_count":15},{"cell_type":"markdown","source":"### Step 3. Create embedding object using our model from huggingface","metadata":{}},{"cell_type":"code","source":"model_kwargs = {'device': 'cpu'}\nencode_kwargs = {'normalize_embeddings': False}\nembedder = HuggingFaceEmbeddings(\n    model_name=model_name,\n    model_kwargs=model_kwargs,\n    encode_kwargs=encode_kwargs\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-02T20:09:43.763667Z","iopub.execute_input":"2025-09-02T20:09:43.764063Z","iopub.status.idle":"2025-09-02T20:09:45.460047Z","shell.execute_reply.started":"2025-09-02T20:09:43.764036Z","shell.execute_reply":"2025-09-02T20:09:45.458919Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"## create PineconeVectorStore object\nvector_store = PineconeVectorStore(index=index, embedding=embedder)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-02T20:09:45.461103Z","iopub.execute_input":"2025-09-02T20:09:45.462026Z","iopub.status.idle":"2025-09-02T20:09:45.467279Z","shell.execute_reply.started":"2025-09-02T20:09:45.461984Z","shell.execute_reply":"2025-09-02T20:09:45.466224Z"}},"outputs":[],"execution_count":17},{"cell_type":"markdown","source":"### Step 4. Populate vector store","metadata":{}},{"cell_type":"code","source":"%%time\n\nprint(\"Total number of documents to upload: \", len(book_docs))\n\ndef batch_list(data, batch_size):\n    \"\"\"Yield successive batch_size-sized chunks from data.\"\"\"\n    for i in range(0, len(data), batch_size):\n        yield data[i:i + batch_size]\n\ni=0\n\nfor batch in batch_list(book_docs, 1000):\n    i+=1\n    print(f\"Uploading batch: {i}\")\n    vector_store.add_documents(batch)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-02T20:13:57.580591Z","iopub.execute_input":"2025-09-02T20:13:57.581076Z","iopub.status.idle":"2025-09-02T20:25:40.719294Z","shell.execute_reply.started":"2025-09-02T20:13:57.581046Z","shell.execute_reply":"2025-09-02T20:25:40.718323Z"}},"outputs":[{"name":"stdout","text":"Total number of documents to upload:  3238\nUploading batch: 1\nUploading batch: 2\nUploading batch: 3\nUploading batch: 4\nCPU times: user 21min 46s, sys: 1min 22s, total: 23min 9s\nWall time: 11min 43s\n","output_type":"stream"}],"execution_count":21}]}